{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/workspaces/aps360-winter2023/nor-smart-speech'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize images to 288 x 288\n",
    "transform = transforms.Compose([transforms.Resize((288, 288)), \n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# SPLIT THE DATA HERE BY SLICING THE DATA!!\n",
    "\n",
    "# TRANSFORMING THE DATA\n",
    "data = torchvision.datasets.ImageFolder(path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT THE DATA (80-10-10 split)\n",
    "train_data, valtest_data = train_test_split(data, test_size=0.2)\n",
    "val_data, test_data = train_test_split(valtest_data, test_size=0.5)\n",
    "\n",
    "# LOADING THE DATA\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=1, num_workers=1, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=1, num_workers=1, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, num_workers=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "for images, labels in train_loader:\n",
    "    # since batch_size = 1, there is only 1 image in `images`\n",
    "    image = images[0]\n",
    "    # place the colour channel at the end, instead of at the beginning\n",
    "    img = np.transpose(image, [1, 2, 0])\n",
    "    # normalize pixel intensity values to [0, 1]\n",
    "    img = img / 2 + 0.5\n",
    "    plt.subplot(3, 5, k+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "\n",
    "    k += 1\n",
    "    if k > 14:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = len(train_loader)\n",
    "validation_examples = len(val_loader)\n",
    "test_examples = len(test_loader)\n",
    "\n",
    "print(\"Number of training examples:\", train_examples)\n",
    "print(\"Number of validation examples:\", validation_examples)\n",
    "print(\"Number of testing examples:\", test_examples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.name = \"CNN\"\n",
    "        self.conv1 = nn.Conv2d(3, 5, 7) # RGB 3 input channels, 5 output channels, 7 x 7 kernel size\n",
    "        self.pool = nn.MaxPool2d(3, 3) # 3 x 3 kernel size, 3 stride\n",
    "        self.conv2 = nn.Conv2d(5, 10, 7) # 5 input channels, 10 output channels, 5 x 5 kernel size\n",
    "        self.pool = nn.MaxPool2d(3, 1) # 3 x 3 kernel size, 1 stride\n",
    "        self.conv3 = nn.Conv2d(10, 15, 3) # 10 input channels, 15 output channels, 3 x 3 kernel size\n",
    "        self.pool = nn.MaxPool2d(3, 3) # 3 x 3 kernel size, 3 stride\n",
    "        self.conv4 = nn.Conv2d(15, 20, 3) # 15 input channels, 20 output channels, 3 x 3 kernel size\n",
    "\n",
    "        # 26 x 26 (20ch) => flat => 26 x 26 x 20\n",
    "        self.fc1 = nn.Linear(20 * 26 * 26, 50) # 50 hidden neurons\n",
    "        self.fc2 = nn.Linear(50, 6) # 6 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 20 * 26 * 26)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.squeeze(1) # flatten to [batch_size]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, train_loader, val_loader, train=False):\n",
    "    if train:\n",
    "        data = train_loader\n",
    "    else:\n",
    "        data = val_loader\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for imgs, labels in data:\n",
    "\n",
    "        #############################################\n",
    "        #To Enable GPU Usage\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "          imgs = imgs.cuda()\n",
    "          labels = labels.cuda()\n",
    "        #############################################\n",
    "        \n",
    "        output = model(imgs)\n",
    "        \n",
    "        # select index with maximum prediction score\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += imgs.shape[0]\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, batch_size=64, learning_rate=0.01, num_epochs=1):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    iters, losses, train_acc, val_acc = [], [], [], []\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "                                           num_workers=1, \n",
    "                                           shuffle=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, \n",
    "                                            num_workers=1, \n",
    "                                            shuffle=True)\n",
    "\n",
    "    # training\n",
    "    n = 0 # the number of iterations\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for imgs, labels in iter(train_loader):\n",
    "\n",
    "            #############################################\n",
    "            #To Enable GPU Usage\n",
    "            if use_cuda and torch.cuda.is_available():\n",
    "              imgs = imgs.cuda()\n",
    "              labels = labels.cuda()\n",
    "            #############################################\n",
    "              \n",
    "            out = model(imgs)             # forward pass\n",
    "\n",
    "            loss = criterion(out, labels) # compute the total loss => CHANGED labels to to labels.float()\n",
    "            loss.backward()               # backward pass (compute parameter updates)\n",
    "            optimizer.step()              # make the updates for each parameter\n",
    "            optimizer.zero_grad()         # a clean up step for PyTorch\n",
    "            n += 1\n",
    "\n",
    "        # save the current training information\n",
    "        iters.append(n)\n",
    "        losses.append(float(loss)/batch_size)             # compute *average* loss\n",
    "        train_acc.append(get_accuracy(model, train_loader, val_loader, train=True)) # compute training accuracy \n",
    "        val_acc.append(get_accuracy(model, train_loader, val_loader, train=False))  # compute validation accuracy\n",
    "        \n",
    "        print(\"Epoch:\", epoch + 1, \"| Training Accuracy:\", train_acc[epoch], \"| Validation Accuracy:\", val_acc[epoch])\n",
    "\n",
    "        # checkpoints\n",
    "        model_path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(model.name, batch_size, learning_rate, epoch)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # plotting\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(iters, losses, label=\"Train\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(iters, train_acc, label=\"Train\")\n",
    "    plt.plot(iters, val_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Training Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
    "    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))\n",
    "    print(\"Total Training Time:\", round(end_time - start_time, 2), \"seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sanity Check: Overfit to a Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
